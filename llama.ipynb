{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f642b11-cfcf-42b4-bbd5-7302cc241d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here2\n",
      "Using device: cuda\n",
      "==============================WARNING: DEPRECATED!==============================\n",
      "WARNING! This version of bitsandbytes is deprecated. Please switch to `pip install bitsandbytes` and the new repo: https://github.com/TimDettmers/bitsandbytes\n",
      "==============================WARNING: DEPRECATED!==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/spourzan/.conda/envs/llamaEnv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "print(\"here\")\n",
    "import torch\n",
    "print(\"here2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "import bitsandbytes as bnb\n",
    "from functools import partial\n",
    "import os\n",
    "# import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f086d8c-a9c2-4b69-933e-449a973b092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from bert_score import score as bert_score\n",
    "# from rouge import Rouge\n",
    "# from tqdm import tqdm  # For progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36b6f82d-840c-43c6-b593-f7a22045747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_name, bnb_config):\n",
    "  n_gpus = torch.cuda.device_count()\n",
    "  max_memory = f'{40960}MB'\n",
    "  # model = AutoModelForCausalLM.from_pretrained(\n",
    "  #     model_name,\n",
    "  #     quantization_config = bnb_config,\n",
    "  #     device_map = \"auto\",\n",
    "  #     max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "  # )\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  return None, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289be6ee-bbe1-430c-ac80-895d3d52d4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (42140, 10)\n"
     ]
    }
   ],
   "source": [
    "# medical_dataset = \"Kabatubare/medical\"\n",
    "# dataset = load_dataset(medical_dataset, split=\"train\")\n",
    "\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('entire_sephora_data.csv')\n",
    "print(\"shape: \", df.shape)\n",
    "\n",
    "# filter out incentivized reviews\n",
    "df = df[df['incentivizedReview'] == False]\n",
    "\n",
    "# combine inputs into a single str and prep outputs\n",
    "# df['inputs'] = df.apply(lambda row: f\"Skin Type: {row['skinType']} | Skin Tone: {row['skinTone']} | Verified Purchaser: {row['verifiedPurchaser']} | Positive Feedback: {row['TotalPositiveFeedbackCount']} | Negative Feedback: {row['TotalNegativeFeedbackCount']} | Recommended: {row['IsRecommended']} | Product: {row['ProductDescription']}\", axis=1)\n",
    "# df['outputs'] = df.apply(lambda row: f\"Review: {row['reviewText']} | Rating: {row['rating']}\", axis=1)\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# train = pd.concat([X_train, y_train], axis=1)\n",
    "# test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "dataset = Dataset.from_pandas(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf0c403-c2ff-4932-8326-bb5b9270a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(row):\n",
    "    formatted_question = f\"<s>[INST]Skin Type: {row['skinType']} | Skin Tone: {row['skinTone']} | Verified Purchaser: {row['verifiedPurchaser']} | Positive Feedback: {row['TotalPositiveFeedbackCount']} | Negative Feedback: {row['TotalNegativeFeedbackCount']} | Recommended: {row['IsRecommended']} | Product: {row['ProductDescription']}[/INST]\"\n",
    "    formatted_answer = f\"Review: {row['reviewText']} | Rating: {row['rating']}</s>\"\n",
    "\n",
    "    # Combine the formatted question and answer into a single column\n",
    "    combined_entry = {'text': f'{formatted_question} {formatted_answer}'}\n",
    "    return combined_entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "516bca21-d505-4981-9d61-9ac75c7a887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 27940\n",
      "Column names are: ['skinType', 'skinTone', 'incentivizedReview', 'verifiedPurchaser', 'reviewText', 'rating', 'TotalPositiveFeedbackCount', 'TotalNegativeFeedbackCount', 'IsRecommended', 'ProductDescription', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "452b2e4c-aba2-4443-b56b-b1275810611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "  conf = model.config\n",
    "  max_length = None\n",
    "  for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "    max_length = getattr(model.config, length_setting, None)\n",
    "    if max_length:\n",
    "      max_length = 1024\n",
    "      print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "  \"\"\"\n",
    "  Tokenizing a batch\n",
    "  \"\"\"\n",
    "  return tokenizer(\n",
    "      batch[\"text\"],\n",
    "      max_length=max_length,\n",
    "      truncation=True,\n",
    "  )\n",
    "\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "  \"\"\"\n",
    "  Format & tokenize it so it is ready for training\n",
    "  :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "  :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "  \"\"\"\n",
    "  # Add prompt to each sample\n",
    "  print(\"Preprocessing dataset...\")\n",
    "  dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "\n",
    "  # Apply preprocessing to each batch of the dataset & remove field other than text column.\n",
    "  _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "  dataset = dataset.map(\n",
    "      _preprocessing_function,\n",
    "      batched=True,\n",
    "      remove_columns=['skinType', 'skinTone', 'incentivizedReview', 'verifiedPurchaser', 'reviewText', 'rating', 'TotalPositiveFeedbackCount', 'TotalNegativeFeedbackCount', 'IsRecommended', 'ProductDescription', '__index_level_0__']\n",
    "  )\n",
    "  # Filter out samples that have input_ids exceeding max_length\n",
    "  dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "  # Shuffle dataset\n",
    "  dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d008981-1aaa-4897-900c-79f5b5153663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e3631ef-779b-4674-8ed1-f400cd132817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f863b3-3d88-4cd0-96d8-c098004f3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    # cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34183785-1388-4614-8dce-c3ca57a97768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c62821-4365-4b6d-8c4d-19487687369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home1/spourzan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/spourzan/.conda/envs/llamaEnv2/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model from HF with user's token and with bitsandbytes config\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = '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')\n",
    "\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "model, tokenizer = load_model(model_name,bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99e3e43f-018b-4812-b32c-8f4abad82b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 27940/27940 [00:04<00:00, 6066.78 examples/s]\n",
      "Map: 100%|██████████| 27940/27940 [00:13<00:00, 2001.07 examples/s]\n",
      "Filter: 100%|██████████| 27940/27940 [00:07<00:00, 3569.17 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 27940\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max_length = 4096\n",
    "\n",
    "dataset = preprocess_dataset(tokenizer, max_length, 42, dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be32d5c-159e-42ca-933b-ec7c8be060af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # # Training parameters\n",
    "    # trainer = Trainer(\n",
    "    #     model=model,\n",
    "    #     train_dataset=dataset,\n",
    "    #     args=TrainingArguments(\n",
    "    #         per_device_train_batch_size=1,\n",
    "    #         gradient_accumulation_steps=4,\n",
    "    #         warmup_steps=2,\n",
    "    #         max_steps=20,\n",
    "    #         learning_rate=2e-4,\n",
    "    #         fp16=True,\n",
    "    #         logging_steps=1,\n",
    "    #         output_dir=\"outputs\",\n",
    "    #         optim=\"paged_adamw_8bit\",\n",
    "    #     ),\n",
    "    #     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    # )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=50,\n",
    "            save_steps=150,\n",
    "            warmup_steps=100,\n",
    "            output_dir=\"outputs2\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "\n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "\n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "\n",
    "    do_train = True\n",
    "\n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "\n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "output_dir = \"results/llama2/final_checkpoint\"\n",
    "train(model, tokenizer, dataset, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735c91c-5e6b-4b65-a69d-8dade4048044",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bf343-c497-4ad6-90bc-45db08f2228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Skin Type: combination | Skin Tone: medium | Verified Purchaser: False | Positive Feedback: 0 | Negative Feedback: 0 | Recommended: True | Product: What it is: A hydrating foundation that delivers buildable coverage for the face and body, resists heat and humidity, and leaves a luminous makeup look.Coverage: MediumFinish: NaturalFormulation: LiquidHighlighted Ingredients: - Squalane- Hyaluronic AcidIngredient Callouts: Free of parabens, formaldehydes, formaldehyde-releasing agents, phthalates, mineral oil, retinyl palmitate, oxybenzone, coal tar, hydroquinone, sulfates SLS & SLES, triclocarban, triclosan, and contains less than one percent synthetic fragrance. What Else You Need to Know: This foundation is formulated with 94 percent natural-origin ingredients and hyaluronic acid. It delivers intense hydration and a second-skin sensation, while providing buildable coverage that evens the skin and blurs imperfections for a wide range of effects, from a flawless no-makeup look to a high-perfection complexion.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, repetition_penalty=1.18, truncation=True, max_length=500, torch_dtype=torch.float16)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fead71-bbae-4b63-a4c0-a74811c4ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704b4e7-3623-4c3f-b2bb-d257fddcb353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“llamaKernel2”",
   "language": "python",
   "name": "llamaenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
